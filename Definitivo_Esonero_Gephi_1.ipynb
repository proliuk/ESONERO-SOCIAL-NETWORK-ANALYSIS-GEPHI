{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Definitivo_Esonero_Gephi_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/proliuk/ESONERO-SOCIAL-NETWORK-ANALYSIS-GEPHI/blob/master/Definitivo_Esonero_Gephi_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ykjvO0lsoWX3",
        "colab": {}
      },
      "source": [
        "#ISTRUZIONI PER COLLEGARE GOOGLE DRIVE\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "drive.mount('/content/drive', force_remount=\"True\")\n",
        "#IMPORTO I VARI FILE NECESSARI \n",
        "path = '/content/drive/My Drive/gephi/combined_csv.csv'\n",
        "pathnomi = '/content/drive/My Drive/gephi/nomi_italiani.txt' \n",
        "pathcognomi = '/content/drive/My Drive/gephi/cognomi.txt' \n",
        "pathstati = '/content/drive/My Drive/gephi/stati.txt' \n",
        "pathcomuni = '/content/drive/My Drive/gephi/comuni.csv'\n",
        "import os\n",
        "#CREIAMO UNA LISTA CONTENENTE TUTTI I NOMI ITALIANI\n",
        "var_nomi_italiani = open(pathnomi,'r').read().split('\\n')\n",
        "for i in range (0, len(var_nomi_italiani)):\n",
        "  check = var_nomi_italiani[i]\n",
        "  var_nomi_italiani[i] = check.capitalize()\n",
        "print(\"Nomi italiani\")\n",
        "print(var_nomi_italiani)\n",
        "print(\"\")\n",
        "#CREIAMO UNA LISTA CONTENENTE TUTTI I COGNOMI ITALIANI\n",
        "var_cognomi = open(pathcognomi,'r').read().split('\\n')\n",
        "print(\"Cognomi italiani\")\n",
        "print(var_cognomi)\n",
        "print(\"\")\n",
        "#CREIAMO UNA LISTA CONTENENTE TUTTI GLI STATI IN ITALIANO\n",
        "var_stati = open(pathstati,'r').read().split('\\t')\n",
        "var_stati.pop(0)\n",
        "var_stati.pop(0)\n",
        "var_stati.pop(0)\n",
        "for i in range(0,len(var_stati)):\n",
        "  if i % 2 != 0:\n",
        "    var_stati[i]=\"\"\n",
        "var_stati_clean=[]\n",
        "[var_stati_clean.append(item) for item in var_stati if item not in var_stati_clean]\n",
        "var_stati_clean.pop(1)\n",
        "print(\"Stati in italiani puliti\")\n",
        "print(var_stati_clean)\n",
        "print(\"\")\n",
        "\n",
        "#CREIAMO UNA LISTA CONTENENTE TUTTI I COMUNI ITALIANI\n",
        "df1 = pd.read_csv('/content/drive/My Drive/gephi/comuni.csv', encoding = \"ISO-8859-1\")\n",
        "list_comuni = df1.values.tolist()\n",
        "var_comuni = []\n",
        "for elem in list_comuni:\n",
        "    var_comuni.extend(elem)\n",
        "print(\"Tutti i comuni italiani\")\n",
        "print(var_comuni)\n",
        "print(\"\")\n",
        "\n",
        "#CREIAMO UNA LISTA CONTENENTE TUTTI NOMI PROPRI IN ITALIANO\n",
        "lista_ita = var_cognomi + var_nomi_italiani + var_comuni + var_stati_clean\n",
        "lista_ita.append(\"Know\") \n",
        "print(f'In tutto i nomi prori in italiano sono -->  {len(lista_ita)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu6GdYueTMpf",
        "colab_type": "text"
      },
      "source": [
        "Blocco che si occupa della pulizia del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eKOUl12Jo-N5",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.snowball import ItalianStemmer\n",
        "import re\n",
        "#Scarichiamo le stopword e importiamo solo quelle italiane\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words(\"italian\"))\n",
        "#Stemming per rimuovere le ultime lettere (Se esse si riferiscono a plurale, cognugazione dei verbi e genere)\n",
        "stemming = SnowballStemmer(\"italian\", ignore_stopwords=False)\n",
        "\n",
        "def apply_cleaning_function_to_list(X):\n",
        "    cleaned_X = []\n",
        "    for element in X:\n",
        "        cleaned_X.append(clean_text(element))\n",
        "    return cleaned_X\n",
        "\n",
        "def clean_text(raw_text):\n",
        "\n",
        "  text = raw_text \n",
        "\n",
        "#VARI CONTROLLI PER LE LETTERE MAIUSCOLO DOPO I SEGNI DI PUNTEGGIATURA. METTE MINUSCOLA LA LETTERA SUCCESSIVA ALLA PUNTEGGIATURA. (Conte. Rubrica --> Conte rubrica)\n",
        "  lista = list(text)\n",
        "  for i in range(0,len(lista)):\n",
        "    if(lista[i]=='\"' and i+1 < len(lista) or lista[i]=='“' and i+1 < len(lista) or lista[i]==\"'\" and i+1 < len(lista)):\n",
        "      if lista[i+1] not in lista_ita:\n",
        "        lista[i+1] = lista[i+1].lower()\n",
        "\n",
        "    if(lista[i]=='.'  and i+1 < len(lista)):\n",
        "      if(lista[i+1]==' ' and i+2 < len(lista)):\n",
        "        if lista[i+2] not in lista_ita:\n",
        "          lista[i+2] = lista[i+2].lower()\n",
        "    \n",
        "    if(lista[i]=='?'  and i+1 < len(lista)):\n",
        "      if(lista[i+1]==' ' and i+2 < len(lista)):\n",
        "        if lista[i+2] not in lista_ita:\n",
        "          lista[i+2] = lista[i+2].lower()\n",
        "    \n",
        "    if(lista[i]=='!'  and i+1 < len(lista)):\n",
        "      if(lista[i+1]==' ' and i+2 < len(lista)):\n",
        "        if lista[i+2] not in lista_ita :\n",
        "          lista[i+2] = lista[i+2].lower()\n",
        "          \n",
        "  text = \"\".join(lista)\n",
        "\n",
        "  #TOCKEN PER DIVIDERE LE PAROLE DEI TITOLI\n",
        "  tokens = nltk.word_tokenize(text, language='italian')\n",
        "  #CONVERSIONE IN MINUSCOLO DELLE PAROLE DI OGNI TITOLO\n",
        "  if tokens[0] not in lista_ita:\n",
        "      tokens[0] = tokens[0].lower()\n",
        "\n",
        "  #ESCLUDIAMO LA PAROLA \"nan\" POICHE' GENERA ERRORE\n",
        "  token_words = [w for w in tokens if w != \"nan\"] \n",
        "  \n",
        "  text_nostopword = token_words\n",
        "  #COMPOSIZIONE NomeMaiuscolo\n",
        "  for i in range (0,len(text_nostopword)):\n",
        "    check = text_nostopword[i]\n",
        "    check3 = \"\"\n",
        "    if(check.lower() not in stops):\n",
        "      if check.islower() :\n",
        "        text_nostopword[i] = stemming.stem(check)\n",
        "      elif (text_nostopword[(i)]).isalpha():\n",
        "        if i+1 < len(text_nostopword):\n",
        "          if not((text_nostopword[(i+1)]).isalpha()):\n",
        "            continue\n",
        "          check2 = text_nostopword[(i+1)]\n",
        "          if check2.istitle():\n",
        "            if check2.lower() not in stops:\n",
        "              text_nostopword[i] = check + check2\n",
        "              text_nostopword[i+1]= \"\"\n",
        "          if i+2 < len(text_nostopword):\n",
        "            if not((text_nostopword[(i+2)]).isalpha()):\n",
        "              continue\n",
        "            check3 = text_nostopword[(i+2)]\n",
        "            if check2.istitle() and check2.lower() not in stops and check3.istitle() and check3.lower() not in stops:\n",
        "              #unisci le due parole\n",
        "              text_nostopword[i] = check + check2 + check3\n",
        "              text_nostopword[i+1]= \"\"\n",
        "              text_nostopword[i+2]= \"\"\n",
        "\n",
        "\n",
        "  text_nostopword = [w for w in text_nostopword if  w.isalpha() ] \n",
        "  #RIMUOVIAMO LE STOPWORDS\n",
        "  text_nostopwords = [w for w in text_nostopword if not w.lower() in stops and len(w)>2 ]\n",
        "\n",
        " \n",
        "  #REJOIN DELLE PAROLE\n",
        "  joined_words = ( \" \".join(text_nostopwords))\n",
        "\n",
        "  #RESUB DELLE VARIE PAROLE SIMILI TRA LORO \n",
        "  joined_words = re.sub(r\"coronavirus\", \"Coronavirus\", joined_words)\n",
        "  joined_words = re.sub(r\"covid\", \"Coronavirus\", joined_words)\n",
        "  joined_words = re.sub(r\"Covid\", \"Coronavirus\", joined_words)\n",
        "  joined_words = re.sub(r\"nelmercatodiwuhan\",\"mercato Wuhan\", joined_words)\n",
        "  joined_words = re.sub(r\"cittàinquaranten\",\"citt quaranten\", joined_words)\n",
        "  joined_words = re.sub(r\"doveènatoilvirus\",\"virus\", joined_words) \n",
        "  #GLI ARTICOLI SENZA TITOLO VERRANNO CANCELLATI\n",
        "  joined_words = re.sub(r\"titol\", \"\", joined_words)\n",
        "  \n",
        "  testopulito = joined_words \n",
        "  \n",
        "  return testopulito\n",
        "\n",
        "\n",
        "#CARICO IL CSV DA PULIRE\n",
        "imdb = pd.read_csv(path)\n",
        "\n",
        "#CREO LA LISTA DEI TITOLI\n",
        "text_to_clean = list(imdb['title'])\n",
        "\n",
        "#PULIZIA DEL CSV\n",
        "for i in range (0, len(text_to_clean)):\n",
        "  text_to_clean[i] = str(text_to_clean[i])\n",
        "cleaned_text = apply_cleaning_function_to_list(text_to_clean)\n",
        "\n",
        "#STAMPIAMO TITOLI PULITI E NON (PER AVERE UN ESEMPIO)\n",
        "print ('Original text:',text_to_clean[5])\n",
        "print ('\\nCleaned text:', cleaned_text[5])\n",
        "\n",
        "#INSERIRE TUTTI I DATI PULITI NEL DATAFREME \n",
        "imdb['title'] = cleaned_text\n",
        "imdb.to_csv('imdb.csv', index=False) #True per avere id prima della frase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nNELO3aTC3x",
        "colab_type": "text"
      },
      "source": [
        "Blocco che permette di creare l'insieme dei nodi e di salvarli all'interno di un CSV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWqXxLqw3lVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open('nodes_table.csv', 'w',newline='') as f:\n",
        "  with open('imdb.csv') as csv_file:\n",
        "    nodi = []\n",
        "    csv_reader = csv.reader(csv_file, delimiter=' ')\n",
        "    thewriter = csv.writer(f) \n",
        "    thewriter.writerow(['Id','Label']) \n",
        "    for row in csv_reader:\n",
        "        for i in range (0,len(row)):\n",
        "          if row[i] not in nodi and len(row[i])>2:\n",
        "            nodi.append(row[i])\n",
        "    nodi.sort()\n",
        "    print(nodi)\n",
        "    for i in range(0,len(nodi)):\n",
        "      thewriter.writerow([i,nodi[i]])\n",
        "print(len(nodi))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNyn32MuGZEd",
        "colab_type": "text"
      },
      "source": [
        "Blocco che permette di creare l'insieme totale degli archi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQsKI7U2FOJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open('edges_table.csv', 'w',newline='') as f:\n",
        "  with open('imdb.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=' ')\n",
        "    lista_edge =[]\n",
        "    line_count = 0\n",
        "    thewriter = csv.writer(f) \n",
        "    thewriter.writerow(['Source','Target']) \n",
        "    for row in csv_reader:\n",
        "        if line_count == 0:\n",
        "            print(f'Column names are {\", \".join(row)}')\n",
        "            line_count += 1\n",
        "        else:\n",
        "            for i in range (0,len(row)):\n",
        "              for c in range (i,len(row)):\n",
        "                if c != i  and len(row[i])>2 and len (row[c])>2:\n",
        "                  thewriter.writerow([nodi.index(row[i]),nodi.index(row[c])])\n",
        "                  edge1 = [nodi.index(row[i]),nodi.index(row[c])]\n",
        "                  lista_edge.append(edge1)\n",
        "            line_count += 1\n",
        "    print(f'Processed {line_count} lines.')\n",
        "    print(lista_edge)\n",
        "    print(\"GLI ARCHI TOTALI SONO : \",len(lista_edge))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfr60gtHGjBD",
        "colab_type": "text"
      },
      "source": [
        "Blocco per creare la lista che contiene gli archi unici."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF7Zh_OcMSTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import progressbar\n",
        "bar = progressbar.ProgressBar(maxval=len(lista_edge), \\\n",
        "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "\n",
        "lista_single = []\n",
        "bar.start()\n",
        "\n",
        "for i in range(0,len(lista_edge)):\n",
        "\n",
        "  bar.update(i+1)\n",
        "\n",
        "  sourcei = lista_edge[i][0]\n",
        "  targeti = lista_edge[i][1]\n",
        "  inverso = [targeti, sourcei]\n",
        "\n",
        "  if lista_edge[i] not in lista_single and inverso not in lista_single:\n",
        "    lista_single.append(lista_edge[i])\n",
        "  \n",
        "bar.finish()\n",
        "print(lista_single)\n",
        "print(\"GLI ARCHI UNICI SONO : \",len(lista_single))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abg_yyjFboRs",
        "colab_type": "text"
      },
      "source": [
        "Blocco che permette di creare la lista con gli archi pesati "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybpP3UW7Nsti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import progressbar\n",
        "bar = progressbar.ProgressBar(maxval=len(lista_single), \\\n",
        "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "\n",
        "lista_pesata = []\n",
        "\n",
        "bar.start()\n",
        "for i in range(0,len(lista_single)):\n",
        "  \n",
        "  bar.update(i+1)\n",
        "\n",
        "  sourcei = lista_single[i][0]\n",
        "  targeti = lista_single[i][1]\n",
        "  inverso = [targeti, sourcei]\n",
        "\n",
        "  peso1 = lista_edge.count(lista_single[i])\n",
        "  peso2 = lista_edge.count(inverso)\n",
        "\n",
        "  peso = peso1 + peso2\n",
        "  lista_pesata.append(peso)\n",
        "\n",
        "bar.finish()\n",
        "print(lista_pesata)\n",
        "print(\"GLI ARCHI PESATI SONO : \",len(lista_pesata))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EzIVH4zG2vi",
        "colab_type": "text"
      },
      "source": [
        "Blocco per copiare la lista degli archi all'interno del CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PlzRoi6QbYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('edges_table.csv', 'w',newline='') as f:\n",
        "\n",
        "  thewriter = csv.writer(f) \n",
        "  thewriter.writerow(['Source','Target','Weight'])\n",
        "\n",
        "  for i in range(0,len(lista_pesata)):\n",
        "    thewriter.writerow([lista_single[i][0],lista_single[i][1],lista_pesata[i]])\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}